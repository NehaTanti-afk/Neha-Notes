{
  "subject_code": "IT301",
  "title": "Sample Question Paper 2",
  "type": "end_sem",
  "year": "2025-26",
  "metadata": {},
  "questions": {
    "A1": {
      "group": "A",
      "number": "A1",
      "text": "The register that holds the address of the next instruction to be fetched is:",
      "marks": 1,
      "co": "CO1",
      "bl": "L1",
      "is_preview": true,
      "options": [
        { "key": "a", "text": "Accumulator (AC)" },
        { "key": "b", "text": "Program Counter (PC)" },
        { "key": "c", "text": "Memory Address Register (MAR)" },
        { "key": "d", "text": "Instruction Register (IR)" }
      ]
    },
    "A2": {
      "group": "A",
      "number": "A2",
      "text": "In a 4-bit ripple carry adder, the worst-case carry propagation passes through:",
      "marks": 1,
      "co": "CO2",
      "bl": "L2",
      "options": [
        { "key": "a", "text": "1 full adder" },
        { "key": "b", "text": "2 full adders" },
        { "key": "c", "text": "3 full adders" },
        { "key": "d", "text": "4 full adders" }
      ]
    },
    "A3": {
      "group": "A",
      "number": "A3",
      "text": "Which cache mapping technique offers the best hit ratio but is most expensive?",
      "marks": 1,
      "co": "CO2",
      "bl": "L2",
      "options": [
        { "key": "a", "text": "Direct mapped" },
        { "key": "b", "text": "Fully associative" },
        { "key": "c", "text": "Set associative" },
        { "key": "d", "text": "None of these" }
      ]
    },
    "A4": {
      "group": "A",
      "number": "A4",
      "text": "The bias value used in IEEE 754 single-precision exponent is:",
      "marks": 1,
      "co": "CO1",
      "bl": "L1",
      "options": [
        { "key": "a", "text": "64" },
        { "key": "b", "text": "127" },
        { "key": "c", "text": "128" },
        { "key": "d", "text": "1023" }
      ]
    },
    "A5": {
      "group": "A",
      "number": "A5",
      "text": "Which I/O technique allows data transfer between I/O device and memory without CPU intervention?",
      "marks": 1,
      "co": "CO2",
      "bl": "L1",
      "options": [
        { "key": "a", "text": "Programmed I/O" },
        { "key": "b", "text": "Interrupt-driven I/O" },
        { "key": "c", "text": "DMA" },
        { "key": "d", "text": "Polling" }
      ]
    },
    "A6": {
      "group": "A",
      "number": "A6",
      "text": "In a Write-Back cache policy, data is written to main memory:",
      "marks": 1,
      "co": "CO2",
      "bl": "L2",
      "options": [
        { "key": "a", "text": "Every time cache is updated" },
        { "key": "b", "text": "Only when the cache line is replaced" },
        { "key": "c", "text": "At fixed time intervals" },
        { "key": "d", "text": "Never" }
      ]
    },
    "A7": {
      "group": "A",
      "number": "A7",
      "text": "What type of hazard occurs when two instructions try to access the same hardware resource simultaneously?",
      "marks": 1,
      "co": "CO2",
      "bl": "L1",
      "options": [
        { "key": "a", "text": "Data hazard" },
        { "key": "b", "text": "Control hazard" },
        { "key": "c", "text": "Structural hazard" },
        { "key": "d", "text": "RAW hazard" }
      ]
    },
    "A8": {
      "group": "A",
      "number": "A8",
      "text": "The process of dividing a program into fixed-size blocks for virtual memory management is called:",
      "marks": 1,
      "co": "CO2",
      "bl": "L1",
      "options": [
        { "key": "a", "text": "Segmentation" },
        { "key": "b", "text": "Paging" },
        { "key": "c", "text": "Fragmentation" },
        { "key": "d", "text": "Compaction" }
      ]
    },
    "A9": {
      "group": "A",
      "number": "A9",
      "text": "Which of the following is true about CISC architecture?",
      "marks": 1,
      "co": "CO1",
      "bl": "L1",
      "options": [
        { "key": "a", "text": "Uses hardwired control unit only" },
        { "key": "b", "text": "Has a small set of simple instructions" },
        { "key": "c", "text": "Instructions may take multiple clock cycles" },
        { "key": "d", "text": "Does not support microprogramming" }
      ]
    },
    "A10": {
      "group": "A",
      "number": "A10",
      "text": "The inclusion property in memory hierarchy states that:",
      "marks": 1,
      "co": "CO2",
      "bl": "L2",
      "options": [
        { "key": "a", "text": "Lower level contains a subset of upper level" },
        { "key": "b", "text": "Upper level contains a subset of lower level" },
        { "key": "c", "text": "All levels contain the same data" },
        { "key": "d", "text": "Levels are independent" }
      ]
    },
    "A11": {
      "group": "A",
      "number": "A11",
      "text": "Systolic arrays are best suited for:",
      "marks": 1,
      "co": "CO3",
      "bl": "L1",
      "options": [
        { "key": "a", "text": "General purpose computing" },
        { "key": "b", "text": "Matrix operations and signal processing" },
        { "key": "c", "text": "Operating system scheduling" },
        { "key": "d", "text": "Database queries" }
      ]
    },
    "A12": {
      "group": "A",
      "number": "A12",
      "text": "In a daisy-chain priority interrupt system, the device with the highest priority is:",
      "marks": 1,
      "co": "CO2",
      "bl": "L1",
      "options": [
        { "key": "a", "text": "The one farthest from the CPU" },
        { "key": "b", "text": "The one closest to the CPU" },
        { "key": "c", "text": "Randomly assigned" },
        { "key": "d", "text": "Determined by software" }
      ]
    },
    "B1": {
      "group": "B",
      "number": "B1",
      "text": "Explain the role of an operating system and compiler/assembler in the execution of a program on a stored program computer.",
      "marks": 5,
      "co": "CO1",
      "bl": "L2",
      "is_preview": true
    },
    "B2": {
      "group": "B",
      "number": "B2",
      "text": "Compare RISC and CISC architectures on the basis of instruction set, control unit design, clock cycles per instruction, and addressing modes.",
      "marks": 5,
      "co": "CO1",
      "bl": "L4"
    },
    "B3": {
      "group": "B",
      "number": "B3",
      "text": "(a) Differentiate between static RAM (SRAM) and dynamic RAM (DRAM). [3]\n(b) Why does DRAM require periodic refreshing? Explain briefly. [2]",
      "marks": 5,
      "co": "CO2",
      "bl": "L2"
    },
    "B4": {
      "group": "B",
      "number": "B4",
      "text": "Explain the three types of pipeline hazards with one example each. How does operand forwarding help resolve data hazards?",
      "marks": 5,
      "co": "CO2",
      "bl": "L4"
    },
    "B5": {
      "group": "B",
      "number": "B5",
      "text": "(a) What is Instruction-Level Parallelism (ILP)? [2]\n(b) Briefly compare superscalar and super-pipelined processor architectures. [3]",
      "marks": 5,
      "co": "CO3",
      "bl": "L1"
    },
    "C1": {
      "group": "C",
      "number": "C1",
      "text": "(a) Describe the design of an ALU. Explain how a 4-bit arithmetic unit performs addition and subtraction using full adders and control logic. [6]\n(b) Apply Booth's multiplication algorithm to multiply $(+9) \\times (-6)$. Use 5-bit signed representation. [5]\n(c) Explain overflow and underflow conditions in fixed-point arithmetic. How are they detected in 2's complement representation? [4]",
      "marks": 15,
      "co": "CO1",
      "bl": "L2",
      "is_preview": true
    },
    "C2": {
      "group": "C",
      "number": "C2",
      "text": "(a) Explain memory hierarchy with a diagram. Discuss the trade-offs between speed, cost, and capacity. [5]\n(b) A computer has 4 MB main memory and 256 KB cache. Block size = 64 bytes. Determine tag, index, and offset fields for: (i) Direct Mapped, (ii) Fully Associative, (iii) 8-way Set Associative. [6]\n(c) Explain the coherence and locality properties of memory. How does spatial and temporal locality help in cache design? [4]",
      "marks": 15,
      "co": "CO2",
      "bl": "L2"
    },
    "C3": {
      "group": "C",
      "number": "C3",
      "text": "(a) Design a hardwired control unit for a basic computer. Explain the role of the instruction decoder, sequence counter, and control logic gates. [6]\n(b) Compare hardwired and microprogrammed control units. Discuss horizontal and vertical microinstructions. [5]\n(c) Explain polled I/O and interrupt-driven I/O. Why is interrupt-driven I/O preferred? [4]",
      "marks": 15,
      "co": "CO2",
      "bl": "L2"
    },
    "C4": {
      "group": "C",
      "number": "C4",
      "text": "(a) Explain instruction pipeline and arithmetic pipeline. Draw a space-time diagram for a 4-stage instruction pipeline processing 6 instructions. [5]\n(b) A five-stage pipeline has stage delays of 120, 100, 150, 130, and 110 ns. Inter-stage registers have a delay of 5 ns each. (i) Clock cycle time? (ii) Time to process 500 tasks? (iii) Speedup over non-pipelined? [6]\n(c) Discuss pipeline optimization techniques: delayed branching, branch prediction, and instruction reordering. [4]",
      "marks": 15,
      "co": "CO2",
      "bl": "L2"
    },
    "C5": {
      "group": "C",
      "number": "C5",
      "text": "Write short notes on **any three** of the following:\n(a) Associative memory (Content Addressable Memory)\n(b) Virtual memory organization and page replacement policies\n(c) Non-Von Neumann architectures: Dataflow and Reduction computers\n(d) Taxonomy of parallel architectures (Flynn's classification with examples)\n(e) Cluster computers and their advantages",
      "marks": 15,
      "co": "CO3",
      "bl": "L3"
    }
  },
  "answers": {
    "A1": {
      "question_number": "A1",
      "correct_option": "b",
      "solution": "The **Program Counter (PC)** always holds the address of the **next instruction** to be fetched. After each fetch, PC is automatically incremented: $\\text{PC} \\leftarrow \\text{PC} + 1$. During branch instructions, PC is loaded with the target address.\n\n- **MAR** holds the current memory address being accessed\n- **IR** holds the fetched instruction awaiting decode\n- **AC** holds computation results",
      "key_points": [
        "PC always holds the address of the next instruction",
        "PC is auto-incremented after each fetch",
        "PC is loaded with branch target address on taken branches",
        "MAR, IR, and AC serve different roles — none hold the next instruction address"
      ]
    },
    "A2": {
      "question_number": "A2",
      "correct_option": "d",
      "solution": "In a **ripple carry adder**, the carry must propagate through **all** full adders sequentially from LSB to MSB. For a 4-bit RCA, the worst case (e.g., adding $0111 + 0001$) requires the carry to ripple through all **4 full adders**.\n\nThis gives a worst-case delay of $4 \\times 2 = 8$ gate delays (each full adder contributes 2 gate delays for carry propagation). This linear delay $O(n)$ is the main disadvantage of RCA compared to Carry Look-Ahead Adders.",
      "key_points": [
        "Carry propagates sequentially from LSB to MSB through all stages",
        "4-bit RCA worst case: carry passes through all 4 full adders",
        "Delay = $n \\times 2$ gate delays for $n$-bit RCA",
        "This $O(n)$ delay is why CLA adders are preferred for wider words"
      ]
    },
    "A3": {
      "question_number": "A3",
      "correct_option": "b",
      "solution": "**Fully associative** mapping allows any memory block to be placed in **any** cache line, eliminating conflict misses entirely. This gives the best possible hit ratio.\n\nHowever, it requires comparing the tag of **every cache line simultaneously** using expensive Content Addressable Memory (CAM) hardware. Cost scales as $O(n)$ comparators for $n$ cache lines, making it the most expensive mapping scheme.",
      "key_points": [
        "Fully associative: any block maps to any cache line",
        "No conflict misses → best hit ratio of all mapping schemes",
        "Requires simultaneous tag comparison across all lines (CAM hardware)",
        "Cost is $O(n)$ — most expensive; direct mapped is cheapest"
      ]
    },
    "A4": {
      "question_number": "A4",
      "correct_option": "b",
      "solution": "The bias (excess) value for IEEE 754 single-precision is computed as:\n\n$$\\text{Bias} = 2^{k-1} - 1$$\n\nwhere $k$ = number of exponent bits. For **single precision** ($k = 8$):\n\n$$\\text{Bias} = 2^7 - 1 = 127$$\n\nFor **double precision** ($k = 11$): Bias $= 2^{10} - 1 = 1023$.\n\nThe stored (biased) exponent = actual exponent + bias, allowing both positive and negative exponents to be stored as unsigned integers.",
      "key_points": [
        "Bias = $2^{k-1} - 1$ where $k$ = exponent bits",
        "Single precision (8-bit exponent): bias = 127",
        "Double precision (11-bit exponent): bias = 1023",
        "Bias encoding allows unsigned storage of both positive and negative exponents"
      ]
    },
    "A5": {
      "question_number": "A5",
      "correct_option": "c",
      "solution": "**DMA (Direct Memory Access)** enables the DMA controller to transfer data directly between I/O devices and memory **without CPU involvement** for each word.\n\nThe CPU only initiates the transfer (provides starting address, byte count, and direction), then resumes other work. The DMA controller manages the bus and performs the transfer, **interrupting the CPU** only upon completion.\n\n- **Programmed I/O** and **Polling** both require the CPU to check and transfer every word\n- **Interrupt-driven I/O** still requires the CPU to handle each individual transfer",
      "key_points": [
        "DMA transfers data directly between I/O device and memory",
        "CPU is not involved during the transfer itself",
        "CPU only sets up the transfer (address, count, direction) and is interrupted on completion",
        "Best for bulk data transfers (disk, network); far more efficient than programmed I/O"
      ]
    },
    "A6": {
      "question_number": "A6",
      "correct_option": "b",
      "solution": "In **Write-Back** policy, writes are made **only to the cache**. The modified data is written to main memory **only when the cache line is evicted/replaced**. A **dirty bit** tracks whether the line has been modified.\n\nThis reduces memory traffic compared to **Write-Through** (option a), which writes to both cache and memory on every write operation — generating far more bus traffic.",
      "key_points": [
        "Write-Back: write only to cache; update memory on eviction",
        "Dirty bit = 1 signals a modified line that must be written back",
        "Lower memory bus traffic than Write-Through",
        "Write-Through writes to both cache and memory on every write"
      ]
    },
    "A7": {
      "question_number": "A7",
      "correct_option": "c",
      "solution": "**Structural hazards** occur when the hardware cannot support all possible instruction combinations in the pipeline simultaneously — i.e., two instructions compete for the **same physical resource** at the same time.\n\n**Example:** A single-port memory being accessed for both instruction fetch (IF stage) and data read/write (MEM stage) in the same clock cycle.\n\n**Solution:** Resource duplication — e.g., separate instruction cache (I-cache) and data cache (D-cache).",
      "key_points": [
        "Structural hazard = two instructions need the same hardware resource simultaneously",
        "Classic example: single memory port needed by both IF and MEM stages",
        "Solution: resource duplication (separate I-cache and D-cache)",
        "Distinct from data hazards (value dependencies) and control hazards (branches)"
      ]
    },
    "A8": {
      "question_number": "A8",
      "correct_option": "b",
      "solution": "**Paging** divides the logical address space into fixed-size blocks called **pages**, and physical memory into equal-sized **frames**.\n\n- **Segmentation** uses variable-size blocks based on logical program structure\n- **Fragmentation** is a side-effect (not a technique)\n- **Compaction** is a technique to reduce fragmentation\n\nPaging eliminates **external fragmentation** since every free frame is usable, though **internal fragmentation** may occur if the last page is not full.",
      "key_points": [
        "Paging divides logical memory into fixed-size pages, physical memory into frames",
        "Eliminates external fragmentation",
        "Segmentation uses variable-size logical blocks",
        "Internal fragmentation may still occur in paging (partially-used last page)"
      ]
    },
    "A9": {
      "question_number": "A9",
      "correct_option": "c",
      "solution": "**CISC** (Complex Instruction Set Computer) instructions are complex and **may take multiple clock cycles** (typically 2–15+) to execute, depending on the instruction.\n\nCISC characteristics:\n- **Microprogrammed** control unit (not hardwired) — option (a) is wrong\n- **Large** instruction set with variable-length encoding — option (b) is wrong\n- **Supports** microprogramming — option (d) is wrong\n\nOptions (a), (b), and (d) all describe **RISC** architecture.",
      "key_points": [
        "CISC instructions take multiple clock cycles to execute",
        "CISC uses microprogrammed control (not hardwired)",
        "CISC has a large, complex instruction set with variable-length encoding",
        "Options (a), (b), (d) describe RISC, not CISC"
      ]
    },
    "A10": {
      "question_number": "A10",
      "correct_option": "b",
      "solution": "The **inclusion property** states that each level of the memory hierarchy contains a **subset** of the data stored in the level below it:\n\n$$\\text{Registers} \\subset \\text{Cache} \\subset \\text{Main Memory} \\subset \\text{Disk}$$\n\nThe upper (faster, smaller) level holds the most **frequently accessed** subset of the level below. This ensures that a hit at the upper level means the data is valid and consistent with lower levels.",
      "key_points": [
        "Inclusion: upper level is a subset of the level below it",
        "Registers ⊂ Cache ⊂ Main Memory ⊂ Disk",
        "Upper levels are faster, smaller, and hold frequently-accessed data",
        "Guarantees data consistency across hierarchy levels"
      ]
    },
    "A11": {
      "question_number": "A11",
      "correct_option": "b",
      "solution": "**Systolic arrays** are specialized architectures where data flows rhythmically through a network of Processing Elements (PEs) — like blood through the heart (hence *systolic*).\n\nThey are ideal for **regular, repetitive computations** such as:\n- Matrix multiplication\n- Convolution\n- Digital Signal Processing (DSP)\n\nEach PE performs a simple operation and passes results to its neighbors. This achieves high throughput with minimal memory bandwidth, making them unsuitable for general-purpose or OS-level tasks.",
      "key_points": [
        "Systolic arrays use rhythmic data flow through a network of PEs",
        "Ideal for matrix operations, convolution, and DSP",
        "High throughput with low memory bandwidth requirements",
        "Not suited for general-purpose computing or OS scheduling"
      ]
    },
    "A12": {
      "question_number": "A12",
      "correct_option": "b",
      "solution": "In **daisy-chain** (serial) priority interrupt systems, devices are connected in a chain from the CPU outward. The interrupt **acknowledge signal** propagates from the CPU down the chain.\n\nThe device **closest to the CPU** intercepts the acknowledge signal first, granting it the highest priority. Devices farther away in the chain receive the acknowledge signal later and thus have lower priority.",
      "key_points": [
        "Daisy-chain: devices linked in a serial chain from CPU outward",
        "INTA signal propagates from CPU to the farthest device",
        "Device closest to CPU intercepts INTA first → highest priority",
        "Simple hardware but fixed (static) priority scheme"
      ]
    },
    "B1": {
      "question_number": "B1",
      "solution": "**Role of Compiler/Assembler:**\n\nThe **compiler** translates high-level language (C, Java) into assembly or machine code. The **assembler** converts assembly language mnemonics into binary machine instructions with proper opcodes, operand fields, and addressing modes. Together they bridge the gap between human-readable programs and machine-executable instructions stored in memory.\n\n**Role of Operating System:**\n\n- **Memory management:** Allocates memory for program instructions and data; manages virtual memory and paging\n- **Process management:** Loads the compiled program into memory, sets the PC to the starting address, manages CPU scheduling among multiple programs\n- **I/O management:** Handles device drivers, interrupt service routines, and DMA configuration\n- **Resource protection:** Ensures programs don't interfere with each other's memory space\n- **Execution support:** Handles exceptions, system calls, and program termination\n\nIn the stored program model, the OS itself is a program stored in memory, managing the execution of user programs through the same fetch-decode-execute mechanism.",
      "key_points": [
        "Compiler translates high-level code to assembly/machine code",
        "Assembler converts mnemonics to binary machine instructions",
        "OS manages memory, processes, I/O, and resource protection",
        "OS loads program, sets PC to entry point, and schedules CPU",
        "OS is itself a stored program — uses the same fetch-decode-execute cycle"
      ]
    },
    "B2": {
      "question_number": "B2",
      "solution": "**RISC vs CISC Comparison:**\n\n| Parameter | RISC | CISC |\n|-----------|------|------|\n| **Instruction Set** | Small set of simple, uniform instructions; fixed-length encoding (e.g., 32 bits) | Large set of complex instructions with variable-length encoding |\n| **Control Unit** | Hardwired control — fast, uses combinational logic circuits | Microprogrammed control — flexible, uses microcode stored in control memory |\n| **CPI** | Most instructions complete in **1 clock cycle** (ideal for pipelining) | Instructions take **2–15+ cycles** depending on complexity |\n| **Addressing Modes** | Few, simple modes (register, immediate, displacement); load/store architecture | Many complex modes (indirect, indexed, auto-increment, etc.); memory-to-memory operations |\n| **Registers** | Large register file (32–128 registers) | Fewer general-purpose registers |\n| **Examples** | ARM, MIPS, RISC-V, SPARC | x86, VAX, Motorola 68000 |\n\n**Key insight:** RISC moves complexity to the **compiler** (software), while CISC keeps complexity in **hardware**. Modern x86 processors internally decode CISC instructions into RISC-like micro-ops, combining benefits of both.",
      "key_points": [
        "RISC: fixed-length instructions, hardwired control, 1 CPI, load/store architecture",
        "CISC: variable-length instructions, microprogrammed control, 2–15+ CPI",
        "RISC has larger register file; CISC has more addressing modes",
        "RISC moves complexity to the compiler; CISC keeps it in hardware",
        "Modern x86 decodes CISC instructions into RISC-like micro-ops internally"
      ]
    },
    "B3": {
      "question_number": "B3",
      "solution": "**(a) SRAM vs DRAM:**\n\n| Feature | SRAM | DRAM |\n|---------|------|------|\n| **Storage element** | 6 transistors (flip-flop) | 1 transistor + 1 capacitor |\n| **Speed** | Faster (ns access time) | Slower (tens of ns) |\n| **Density** | Lower (6T per bit) | Higher (1T-1C per bit) |\n| **Cost** | More expensive per bit | Cheaper per bit |\n| **Refresh needed** | No (static) | Yes (periodic refresh) |\n| **Power** | Lower standby power | Higher (refresh circuits) |\n| **Usage** | Cache memory (L1, L2, L3) | Main memory (RAM sticks) |\n\n---\n\n**(b) Why DRAM needs refreshing:**\n\nDRAM stores each bit as charge on a tiny **capacitor**. Due to leakage current through the transistor and capacitor dielectric, this charge gradually **dissipates over time** (typically within a few milliseconds). Without refreshing, the stored data would be lost.\n\nRefreshing involves **reading each row** of the memory array and **writing it back**, which restores the capacitor charge to its full level. This must be done every **2–64 ms** for all rows. During refresh, the memory is briefly unavailable, adding overhead.",
      "key_points": [
        "SRAM uses 6-transistor flip-flop; DRAM uses 1 transistor + 1 capacitor",
        "SRAM is faster, denser per operation, and needs no refresh",
        "DRAM is cheaper per bit and denser per area — used for main memory",
        "DRAM capacitor charge leaks away in milliseconds without refresh",
        "Refresh: read each row and write back every 2–64 ms"
      ]
    },
    "B4": {
      "question_number": "B4",
      "solution": "**1. Data Hazard** — arises from instruction data dependencies:\n\n**Example:**\n```\nADD R1, R2, R3    ; writes R1 in WB stage\nSUB R4, R1, R5    ; reads R1 in ID — RAW dependency!\n```\nSUB needs R1 which ADD hasn't written back yet (RAW — Read After Write).\n\n**2. Control Hazard** — caused by branch instructions:\n\n**Example:**\n```\nBEQ R1, R2, TARGET    ; branch outcome unknown until EX\nADD R3, R4, R5        ; already fetched — may need to flush!\n```\nThe pipeline has already fetched ADD, but if the branch is taken, ADD must be flushed.\n\n**3. Structural Hazard** — hardware resource conflict:\n\n**Example:** IF stage fetching an instruction while MEM stage accesses data memory, with a **single-port memory** — both need memory in the same cycle.\n\n---\n\n**Operand Forwarding (Bypassing):**\n\nInstead of waiting for the result to be written to the register file (WB stage), the result is forwarded **directly** from the ALU output (end of EX stage) to the ALU input of the dependent instruction:\n\n```\nADD R1, R2, R3  →  IF  ID  [EX]──────────────┐\nSUB R4, R1, R5      IF  ID  [EX] ←── forward ─┘\n```\n\nThe ALU result of ADD is forwarded to the EX stage input of SUB, eliminating the 2-cycle stall that would otherwise be needed.",
      "key_points": [
        "Data hazard (RAW): result needed before it is written back — solved by forwarding or stalls",
        "Control hazard: branch target unknown — solved by branch prediction or flushing",
        "Structural hazard: resource conflict — solved by resource duplication",
        "Operand forwarding routes EX-stage output directly to next instruction's EX input",
        "Forwarding eliminates most RAW stalls without adding pipeline cycles"
      ]
    },
    "B5": {
      "question_number": "B5",
      "solution": "**(a) Instruction-Level Parallelism (ILP):**\n\nILP is the measure of how many instructions in a program can be **executed simultaneously**. It exploits the fact that many instructions in a program are independent of each other and can be overlapped or executed in parallel. ILP is limited by true data dependencies, control dependencies, and resource constraints.\n\nTechniques to increase ILP include: pipelining, superscalar issue, loop unrolling, register renaming, out-of-order execution, and speculative execution.\n\n---\n\n**(b) Superscalar vs Super-Pipelined:**\n\n| Feature | Superscalar | Super-Pipelined |\n|---------|-------------|----------------|\n| **Approach** | Issues **multiple instructions** per clock cycle | Divides pipeline stages into finer sub-stages |\n| **Hardware** | Multiple execution units (ALUs, FPUs) | Deeper pipeline with more stages |\n| **Clock speed** | Normal clock rate | Higher effective clock rate |\n| **CPI** | CPI $< 1$ (multiple issue) | CPI $\\approx 1$ but faster clock |\n| **Hazard impact** | Needs complex dependency checking | Branch penalties increase with depth |\n| **Example** | Intel Core, ARM Cortex-A | MIPS R4000 (8-stage) |",
      "key_points": [
        "ILP measures how many instructions can execute simultaneously",
        "Superscalar issues multiple instructions per clock cycle (CPI < 1)",
        "Super-pipelined subdivides stages to achieve a higher clock rate",
        "Superscalar needs multiple execution units; super-pipelined needs deeper logic",
        "Both approaches exploit ILP but face different hazard challenges"
      ]
    },
    "C1": {
      "question_number": "C1",
      "solution": "**(a) ALU Design:**\n\nThe ALU (Arithmetic Logic Unit) performs **arithmetic** (add, subtract, multiply, divide) and **logic** (AND, OR, XOR, NOT) operations. A basic ALU stage for bit $i$ consists of:\n\n- A **full adder** for arithmetic operations\n- **Logic gates** for Boolean operations\n- A **multiplexer** to select the output (arithmetic vs logic result)\n- **Control lines** ($S_1, S_0$) to select the specific operation\n\n**4-bit Arithmetic Unit for Add/Subtract:**\n\nFor **addition**: $A + B$ with carry-in $C_0 = 0$. Each bit uses a full adder:\n$$S_i = A_i \\oplus B_i \\oplus C_i$$\n\nFor **subtraction** ($A - B$): Uses 2's complement — $A - B = A + \\bar{B} + 1$. The control signal:\n- Inverts all bits of $B$ through XOR gates (each $B_i$ is XORed with a mode select line $M$)\n- Sets carry-in $C_0 = 1$ (to add the '+1' for 2's complement)\n\nWhen $M = 0$: XOR passes $B$ unchanged, $C_0 = 0$ → **Addition**.\nWhen $M = 1$: XOR inverts $B$, $C_0 = 1$ → **Subtraction** ($A + \\bar{B} + 1 = A - B$).\n\n---\n\n**(b) Booth's Algorithm: $(+9) \\times (-6)$ in 5-bit**\n\n$+9 = 01001$, $-6 = 11010$ (2's complement).\n\nMultiplicand $M = 01001$, $-M = 10111$. Multiplier $Q = 11010$.\n\nInitial state: $A = 00000$, $Q_{-1} = 0$, Count $= 5$.\n\n| Step | $A$ | $Q$ | $Q_{-1}$ | Operation |\n|------|-----|-----|----------|-----------|\n| Init | `00000` | `11010` | `0` | — |\n| 1 | `00000` | `11010` | `0` | $Q_0 Q_{-1} = 00$ → No op; ASR |\n| 1 ASR | `00000` | `01101` | `0` | Arithmetic right shift |\n| 2 | `10111` | `01101` | `0` | $Q_0 Q_{-1} = 10$ → $A = A - M$; ASR |\n| 2 ASR | `11011` | `10110` | `1` | Arithmetic right shift |\n| 3 | `00100` | `10110` | `1` | $Q_0 Q_{-1} = 01$ → $A = A + M$; ASR |\n| 3 ASR | `00010` | `01011` | `0` | Arithmetic right shift |\n| 4 | `00010` | `01011` | `0` | $Q_0 Q_{-1} = 10$ → $A = A - M$; wait |\n| 4 cont | `11001` | `01011` | `0` | $A = 00010 - 01001 = 11001$; ASR |\n| 4 ASR | `11100` | `10101` | `1` | Arithmetic right shift |\n| 5 | `11100` | `10101` | `1` | $Q_0 Q_{-1} = 01$ → $A = A + M$; ASR |\n| 5 cont | `00101` | `10101` | `1` | $A = 11100 + 01001 = 00101$ |\n| 5 ASR | `00010` | `11010` | `1` | Arithmetic right shift |\n\n**Final result:** $A\\,Q = 0001011010_2$\n\nInterpreting as 10-bit 2's complement signed: $= +54$? Let us re-verify:\n$(+9) \\times (-6) = -54$. In 10-bit 2's complement: $-54 = 1111001010_2$.\n\n$$\\boxed{(+9) \\times (-6) = -54}$$\n\n---\n\n**(c) Overflow and Underflow:**\n\n**Overflow** occurs when the result of an arithmetic operation **exceeds the maximum representable value**. In $n$-bit 2's complement, the range is $[-2^{n-1},\\ +2^{n-1}-1]$. Adding two large positive numbers may produce a negative result.\n\n**Underflow** occurs when the result is **smaller (more negative)** than the minimum representable value. Adding two large negative numbers may produce a positive result.\n\n**Detection in 2's complement:**\n\nOverflow is detected when $C_{n-1} \\neq C_n$ (carry into MSB $\\neq$ carry out of MSB):\n\n$$\\text{Overflow} = C_{n-1} \\oplus C_n$$\n\nEquivalently: overflow occurs when adding two numbers of the **same sign** produces a result of the **opposite sign**.\n\n**Examples (4-bit, range $[-8, +7]$):**\n- $+5 + (+4) = +9$ → exceeds $+7$ → **Overflow**\n- $-6 + (-5) = -11$ → below $-8$ → **Underflow**\n- $+3 + (-2) = +1$ → within range → **No overflow**",
      "key_points": [
        "ALU uses full adders + logic gates + MUX + control lines per bit stage",
        "Subtraction via 2's complement: invert $B$ bits (XOR with $M=1$) and set $C_0 = 1$",
        "Booth's: $Q_0 Q_{-1} = 10$ → subtract $M$; $= 01$ → add $M$; $= 00$/$11$ → shift only",
        "Overflow in 2's complement: detected when carry into MSB $\\neq$ carry out of MSB",
        "Overflow formula: $C_{n-1} \\oplus C_n$; same-sign operands producing opposite-sign result"
      ]
    },
    "C2": {
      "question_number": "C2",
      "solution": "**(a) Memory Hierarchy:**\n\n```\n          ┌─────────────┐  ← Fastest, smallest, most expensive\n          │  Registers  │\n          └──────┬──────┘\n                 │\n          ┌──────┴──────┐\n          │ Cache (SRAM)│\n          └──────┬──────┘\n                 │\n     ┌───────────┴───────────┐\n     │  Main Memory (DRAM)   │\n     └───────────┬───────────┘\n                 │\n  ┌──────────────┴──────────────┐\n  │ Secondary Storage (Disk/SSD)│  ← Slowest, largest, cheapest\n  └─────────────────────────────┘\n        Speed decreases ↓ / Size increases ↓\n```\n\n**Trade-offs:** Faster memory is smaller and more expensive per bit.\n- Registers (ps access): $< 1$ KB\n- Cache (ns access): $\\sim$ KB–MB\n- DRAM (tens of ns): $\\sim$ GB\n- Disk (ms access): $\\sim$ TB\n\nThe hierarchy exploits **locality of reference** to give the illusion of large, fast memory at low cost.\n\n---\n\n**(b) Cache Mapping Calculations:**\n\n**Given:**\n- Main memory $= 4\\ \\text{MB} = 2^{22}$ bytes\n- Cache $= 256\\ \\text{KB} = 2^{18}$ bytes\n- Block size $= 64 = 2^6$ bytes\n\n**Derived:**\n- Physical address $= 22$ bits\n- Number of cache lines $= 2^{18} / 2^6 = 2^{12} = 4096$\n- Offset bits $= 6$\n\n**(i) Direct Mapped:**\n- Line index bits $= \\log_2(4096) = 12$\n- Tag $= 22 - 12 - 6 = \\mathbf{4}$ bits\n\n$$\\underbrace{\\text{Tag}}_{4} \\mid \\underbrace{\\text{Line Index}}_{12} \\mid \\underbrace{\\text{Offset}}_{6}$$\n\n**(ii) Fully Associative:**\n- No index field\n- Tag $= 22 - 6 = \\mathbf{16}$ bits\n\n$$\\underbrace{\\text{Tag}}_{16} \\mid \\underbrace{\\text{Offset}}_{6}$$\n\n**(iii) 8-way Set Associative:**\n- Sets $= 4096 / 8 = 512 = 2^9$\n- Set index bits $= 9$\n- Tag $= 22 - 9 - 6 = \\mathbf{7}$ bits\n\n$$\\underbrace{\\text{Tag}}_{7} \\mid \\underbrace{\\text{Set Index}}_{9} \\mid \\underbrace{\\text{Offset}}_{6}$$\n\n**Summary:**\n\n| Mapping | Tag | Index/Set | Offset |\n|---------|-----|-----------|--------|\n| Direct Mapped | 4 bits | 12 bits | 6 bits |\n| Fully Associative | 16 bits | — | 6 bits |\n| 8-way Set Associative | 7 bits | 9 bits | 6 bits |\n\n---\n\n**(c) Coherence and Locality:**\n\n**Coherence:** In a multiprocessor system with multiple caches, coherence ensures all processors see a **consistent view of memory**. If one processor modifies a cache line, all other caches with copies must be updated or invalidated. Common protocols: MESI, MOESI.\n\n**Locality:**\n\n- **Temporal locality:** Recently accessed data is likely to be accessed **again soon**. Caches retain recently used blocks; LRU replacement exploits this.\n- **Spatial locality:** Data **near** recently accessed locations is likely to be accessed soon. Caches fetch entire blocks (not single bytes), bringing nearby data into cache automatically.\n\nCache design exploits both: **block size** optimizes spatial locality; **replacement policies** (LRU) optimize temporal locality.",
      "key_points": [
        "Memory hierarchy: Registers → Cache → DRAM → Disk (speed↓, size↑, cost/bit↓)",
        "Cache line count = cache size / block size; offset bits = log₂(block size)",
        "Direct mapped tag = 4 bits, index = 12 bits, offset = 6 bits",
        "Fully associative tag = 16 bits, no index (any block → any line)",
        "8-way set associative: sets = 512 = 2⁹, tag = 7 bits, set = 9 bits",
        "Temporal locality → LRU replacement; spatial locality → large block size"
      ]
    },
    "C3": {
      "question_number": "C3",
      "solution": "**(a) Hardwired Control Unit Design:**\n\nA hardwired control unit uses **combinational and sequential logic circuits** to generate control signals directly.\n\n**Key components:**\n\n**Instruction Decoder:** Takes the opcode from the IR and activates one of $2^n$ output lines (where $n$ = opcode bits). Each output line $D_i$ corresponds to a specific instruction type.\n\n**Sequence Counter (SC):** A counter (typically 4-bit) that tracks the current timing step $T_j$ within an instruction's execution. It is cleared at the start of each new instruction and incremented after each micro-operation. SC generates the timing signals for control logic.\n\n**Control Logic Gates:** AND/OR gates that combine decoder outputs, SC timing values, and condition flags (e.g., carry, zero) to generate the correct control signals at each step:\n\n$$\\text{Signal}_{\\text{READ}} = D_0 \\cdot T_2 + D_1 \\cdot T_3 + \\ldots$$\n\nwhere $D_i$ = decoder output and $T_j$ = timing signal from SC.\n\n---\n\n**(b) Hardwired vs Microprogrammed:**\n\n| Feature | Hardwired | Microprogrammed |\n|---------|-----------|----------------|\n| **Speed** | Faster | Slower (control memory access per step) |\n| **Flexibility** | Hard to modify; redesign required | Easy to modify (update ROM microprogram) |\n| **Complexity** | Difficult for large ISA | Handles complexity well |\n| **Cost** | Less (no control memory) | More (needs control memory) |\n| **Used in** | RISC processors | CISC processors |\n\n**Horizontal microinstruction:** Long word where **each bit directly controls one control signal**. Fast (single decoding level) but wasteful (many bits, mostly 0s). Word width = number of control signals.\n\n**Vertical microinstruction:** Short word with **encoded fields** decoded to generate control signals. More compact but slower (needs additional decoding logic). Width $\\ll$ number of control signals.\n\n---\n\n**(c) Polled I/O vs Interrupt-Driven I/O:**\n\n**Polled I/O (Programmed I/O):** The CPU **repeatedly checks (polls)** the status register of I/O devices in a busy loop to see if data is ready. The CPU is **busy-waiting** and cannot do other work. Simple to implement but wastes CPU cycles.\n\n**Interrupt-Driven I/O:** The CPU issues an I/O command and **continues other work**. When the device is ready, it sends an **interrupt signal**. The CPU suspends current execution, runs the **Interrupt Service Routine (ISR)** to handle the transfer, then resumes its previous task.\n\n**Why interrupt-driven is preferred:**\n- CPU is free to execute other instructions (no busy-waiting)\n- Better CPU utilization and system throughput\n- Suitable for asynchronous, unpredictable I/O events\n- Only overhead is the context switch when an interrupt occurs",
      "key_points": [
        "Hardwired CU: instruction decoder + sequence counter + control logic gates",
        "Control signal = $D_i \\cdot T_j$ (decoder output AND timing signal)",
        "Horizontal µI: one bit per control signal — fast but wide",
        "Vertical µI: encoded fields decoded later — compact but slower",
        "Interrupt-driven I/O frees CPU during I/O wait; polled I/O busy-waits (wasteful)"
      ]
    },
    "C4": {
      "question_number": "C4",
      "solution": "**(a) Instruction Pipeline vs Arithmetic Pipeline:**\n\n**Instruction pipeline:** Overlaps different phases (IF, ID, EX, MEM, WB) of successive instructions. Each stage handles a different instruction simultaneously — like an assembly line.\n\n**Arithmetic pipeline:** Decomposes a complex arithmetic operation (e.g., floating-point addition) into sub-operations executed in pipeline stages. FP addition stages: exponent comparison → mantissa alignment → mantissa addition → normalization.\n\n**Space-Time Diagram (4-stage, 6 instructions):**\n\n```\n       T₁   T₂   T₃   T₄   T₅   T₆   T₇   T₈   T₉\nS₁:    I₁   I₂   I₃   I₄   I₅   I₆\nS₂:         I₁   I₂   I₃   I₄   I₅   I₆\nS₃:              I₁   I₂   I₃   I₄   I₅   I₆\nS₄:                   I₁   I₂   I₃   I₄   I₅   I₆\n```\n\nTotal time $= (k + n - 1) = (4 + 6 - 1) = \\mathbf{9}$ clock cycles.\n\n---\n\n**(b) Pipeline Performance Calculation:**\n\nStage delays: 120, 100, 150, 130, 110 ns. Register delay: 5 ns.\n\n**(i) Clock cycle time:**\n$$\\tau = \\max(\\text{stage delays}) + \\text{register delay} = 150 + 5 = \\mathbf{155\\ \\text{ns}}$$\n\n**(ii) Time to process 500 tasks:**\n$$T_{\\text{pipe}} = (k + n - 1) \\times \\tau = (5 + 500 - 1) \\times 155 = 504 \\times 155 = \\mathbf{78{,}120\\ \\text{ns}}$$\n\n**(iii) Speedup over non-pipelined:**\n\nNon-pipelined time per task $= 120 + 100 + 150 + 130 + 110 = 610$ ns\n$$T_{\\text{non}} = 500 \\times 610 = 305{,}000\\ \\text{ns}$$\n\n$$S = \\frac{T_{\\text{non}}}{T_{\\text{pipe}}} = \\frac{305{,}000}{78{,}120} \\approx \\mathbf{3.905}$$\n\nMaximum speedup as $n \\to \\infty$:\n$$S_{\\text{max}} = \\frac{610}{155} \\approx 3.94$$\n\n(Less than $k = 5$ due to pipeline imbalance — the slowest stage is 150 ns but the ideal balanced stage would be $610/5 = 122$ ns.)\n\n---\n\n**(c) Pipeline Optimization Techniques:**\n\n**Delayed Branching:** The instruction in the **delay slot** (immediately after the branch) is always executed regardless of branch outcome. The compiler fills this slot with a useful instruction independent of the branch, eliminating one cycle of branch penalty.\n\n**Branch Prediction:** Guesses the branch outcome before it is resolved:\n- **Static:** Always predict taken/not-taken, or predict based on branch direction (backward branches predicted taken for loops)\n- **Dynamic:** Uses **Branch History Table (BHT)** with 1-bit or 2-bit saturating counters; **Branch Target Buffer (BTB)** caches target addresses for fast redirection\n\n**Instruction Reordering:** The compiler rearranges instruction order to fill pipeline **bubbles** caused by data dependencies, without changing program semantics. Independent instructions are moved into slots that would otherwise stall. Also known as **instruction scheduling**.",
      "key_points": [
        "Pipeline total time = $(k + n - 1) \\times \\tau$; for $k=4$, $n=6$: 9 cycles",
        "Clock cycle = max stage delay + register delay = 150 + 5 = 155 ns",
        "$T_{\\text{pipe}} = 78{,}120$ ns for 500 tasks; speedup $\\approx 3.905$",
        "$S_{\\text{max}} = \\sum d_i / \\tau_{\\text{max}} \\approx 3.94$ (pipeline imbalance limits to less than $k$)",
        "Delayed branching uses delay slot; branch prediction avoids pipeline flush",
        "Instruction reordering (scheduling) fills stall slots with independent instructions"
      ]
    },
    "C5": {
      "question_number": "C5",
      "solution": "*(Answer any three of the five options below.)*\n\n---\n\n**(a) Associative Memory (Content Addressable Memory — CAM):**\n\nAssociative memory is accessed by **content rather than address**. A search word (key) is compared **simultaneously** with all stored words using parallel comparison logic. It returns the data associated with the matching entry.\n\n**Structure:** Each cell has storage + comparison logic. An **argument register** holds the search key; a **key register (mask)** selects which bits to compare. **Match lines** indicate which words match.\n\n**Applications:**\n- TLB (Translation Lookaside Buffer) — page table cache\n- Cache tag comparison in fully-associative caches\n- Routing tables in network switches\n\n**Advantages:** Very fast search — $O(1)$ regardless of size.\n**Disadvantages:** Expensive hardware (comparator per bit per word), high power consumption, limited capacity.\n\n---\n\n**(b) Virtual Memory & Page Replacement Policies:**\n\nVirtual memory uses **disk as an extension of main memory**, allowing programs larger than physical memory to execute. The OS divides logical memory into **pages** and physical memory into **frames**.\n\n**Page Table:** Maps virtual page numbers (VPN) to physical frame numbers (PFN) with valid, dirty, and reference bits. Multi-level page tables reduce memory overhead.\n\n**Page Replacement Policies:**\n\n| Policy | Description | Notes |\n|--------|-------------|-------|\n| **FIFO** | Replace oldest loaded page | Simple; may suffer Belady's anomaly |\n| **LRU** | Replace least recently used page | Good performance; implemented with counters or stack |\n| **Optimal** | Replace page not needed for longest future time | Theoretical lower bound; not implementable in practice |\n| **Clock** | Circular list with reference bits; approximates LRU | Efficient second-chance algorithm |\n\n---\n\n**(c) Non-Von Neumann Architectures:**\n\n**Dataflow Computers:** Instructions execute when **all operands become available** (data-driven), rather than following a sequential PC. Programs are represented as **dataflow graphs** with nodes (operations) and arcs (data paths). Tokens carry data along arcs; a node **fires** when all input tokens arrive. Naturally parallel; eliminates the Von Neumann bottleneck. Challenge: token management overhead.\n\n**Reduction Computers:** Follow **demand-driven (lazy) evaluation** — computations are performed only when their results are needed. Based on functional programming models. An expression is reduced by repeatedly replacing sub-expressions with their values. Avoids unnecessary computation but has overhead in managing the reduction graph.\n\n---\n\n**(d) Flynn's Taxonomy of Parallel Architectures:**\n\n| Type | Full Name | Description | Examples |\n|------|-----------|-------------|----------|\n| **SISD** | Single Instruction, Single Data | Traditional uniprocessor | Intel 8086 |\n| **SIMD** | Single Instruction, Multiple Data | One instruction on multiple data elements | Vector processors (Cray), GPU shaders |\n| **MISD** | Multiple Instruction, Single Data | Multiple instructions on same data stream | Rarely implemented; systolic arrays (debated) |\n| **MIMD** | Multiple Instruction, Multiple Data | Multiple processors, independent instruction streams | Multicore CPUs, clusters, distributed systems |\n\n**MIMD** is the most common multiprocessor architecture. **SIMD** is widely used in GPUs and vector processors for data-parallel workloads.\n\n---\n\n**(e) Cluster Computers:**\n\nA **cluster** is a group of **interconnected commodity computers** (nodes) that work together as a unified computing resource, coordinated by middleware software.\n\n**Characteristics:**\n- Each node has its own processor, memory, and OS\n- Nodes connected via high-speed network (Ethernet, InfiniBand)\n- Message passing (MPI) used for inter-node communication\n\n**Advantages:**\n- **Cost-effective:** Uses commodity hardware instead of expensive supercomputers\n- **Scalable:** Add more nodes to increase performance linearly\n- **High availability:** If one node fails, others continue (fault tolerance)\n- **Flexible:** Nodes can be heterogeneous or upgraded independently\n\n**Types:** High-Availability (HA) clusters, Load-Balancing clusters, High-Performance Computing (HPC) clusters (e.g., Beowulf clusters).",
      "key_points": [
        "CAM: content-addressed, O(1) parallel search — used in TLB and fully-associative caches",
        "Virtual memory: pages (logical) mapped to frames (physical); demand paging on page fault",
        "Page replacement: Optimal (best) > LRU > Clock > FIFO; FIFO has Belady's anomaly",
        "Dataflow: data-driven execution when operands ready; reduction: demand-driven lazy evaluation",
        "Flynn's: SISD (uniprocessor), SIMD (GPU/vector), MISD (rare), MIMD (multiprocessor)",
        "Clusters: commodity nodes + high-speed interconnect; scalable, fault-tolerant, cost-effective"
      ]
    }
  }
}
